x-shared-env: &shared-api-worker-env
  LOG_LEVEL: ${LOG_LEVEL}
  DEBUG: ${DEBUG}
  FLASK_DEBUG: ${FLASK_DEBUG}
  SECRET_KEY: ${SECRET_KEY}
  INIT_PASSWORD: ${INIT_PASSWORD}
  CONSOLE_WEB_URL: ${CONSOLE_WEB_URL}
  CONSOLE_API_URL: ${CONSOLE_API_URL}
  SERVICE_API_URL: ${SERVICE_API_URL}
  APP_WEB_URL: ${APP_WEB_URL}
  CHECK_UPDATE_URL: ${CHECK_UPDATE_URL}
  OPENAI_API_BASE: ${OPENAI_API_BASE}
  FILES_URL: ${FILES_URL}
  FILES_ACCESS_TIMEOUT: ${FILES_ACCESS_TIMEOUT}
  APP_MAX_ACTIVE_REQUESTS: ${APP_MAX_ACTIVE_REQUESTS}
  MIGRATION_ENABLED: ${MIGRATION_ENABLED}
  DEPLOY_ENV: ${DEPLOY_ENV}
  DIFY_BIND_ADDRESS: ${DIFY_BIND_ADDRESS}
  DIFY_PORT: ${DIFY_PORT}
  SERVER_WORKER_AMOUNT: ${SERVER_WORKER_AMOUNT}
  SERVER_WORKER_CLASS: ${SERVER_WORKER_CLASS}
  CELERY_WORKER_CLASS: ${CELERY_WORKER_CLASS}
  GUNICORN_TIMEOUT: ${GUNICORN_TIMEOUT}
  CELERY_WORKER_AMOUNT: ${CELERY_WORKER_AMOUNT}
  CELERY_AUTO_SCALE: ${CELERY_AUTO_SCALE}
  CELERY_MAX_WORKERS: ${CELERY_MAX_WORKERS}
  CELERY_MIN_WORKERS: ${CELERY_MIN_WORKERS}
  API_TOOL_DEFAULT_CONNECT_TIMEOUT: ${API_TOOL_DEFAULT_CONNECT_TIMEOUT}
  API_TOOL_DEFAULT_READ_TIMEOUT: ${API_TOOL_DEFAULT_READ_TIMEOUT}
  DB_USERNAME: ${DB_USERNAME}
  DB_PASSWORD: ${DB_PASSWORD}
  DB_HOST: ${DB_HOST}
  DB_PORT: ${DB_PORT}
  DB_DATABASE: ${DB_DATABASE}
  SQLALCHEMY_POOL_SIZE: ${SQLALCHEMY_POOL_SIZE}
  SQLALCHEMY_POOL_RECYCLE: ${SQLALCHEMY_POOL_RECYCLE}
  SQLALCHEMY_ECHO: ${SQLALCHEMY_ECHO}
  REDIS_HOST: ${REDIS_HOST}
  REDIS_PORT: ${REDIS_PORT}
  REDIS_USERNAME: ${REDIS_USERNAME}
  REDIS_PASSWORD: ${REDIS_PASSWORD}
  REDIS_USE_SSL: ${REDIS_USE_SSL}
  REDIS_DB: 0
  CELERY_BROKER_URL: ${CELERY_BROKER_URL}
  BROKER_USE_SSL: ${BROKER_USE_SSL}
  WEB_API_CORS_ALLOW_ORIGINS: ${WEB_API_CORS_ALLOW_ORIGINS}
  CONSOLE_CORS_ALLOW_ORIGINS: ${CONSOLE_CORS_ALLOW_ORIGINS}
  STORAGE_TYPE: ${STORAGE_TYPE}
  STORAGE_LOCAL_PATH: storage
  S3_USE_AWS_MANAGED_IAM: ${S3_USE_AWS_MANAGED_IAM}
  S3_ENDPOINT: ${S3_ENDPOINT}
  S3_BUCKET_NAME: ${S3_BUCKET_NAME}
  S3_ACCESS_KEY: ${S3_ACCESS_KEY}
  S3_SECRET_KEY: ${S3_SECRET_KEY}
  S3_REGION: ${S3_REGION}
  AZURE_BLOB_ACCOUNT_NAME: ${AZURE_BLOB_ACCOUNT_NAME}
  AZURE_BLOB_ACCOUNT_KEY: ${AZURE_BLOB_ACCOUNT_KEY}
  AZURE_BLOB_CONTAINER_NAME: ${AZURE_BLOB_CONTAINER_NAME}
  AZURE_BLOB_ACCOUNT_URL: ${AZURE_BLOB_ACCOUNT_URL}
  GOOGLE_STORAGE_BUCKET_NAME: ${GOOGLE_STORAGE_BUCKET_NAME}
  GOOGLE_STORAGE_SERVICE_ACCOUNT_JSON_BASE64: ${GOOGLE_STORAGE_SERVICE_ACCOUNT_JSON_BASE64}
  ALIYUN_OSS_BUCKET_NAME: ${ALIYUN_OSS_BUCKET_NAME}
  ALIYUN_OSS_ACCESS_KEY: ${ALIYUN_OSS_ACCESS_KEY}
  ALIYUN_OSS_SECRET_KEY: ${ALIYUN_OSS_SECRET_KEY}
  ALIYUN_OSS_ENDPOINT: ${ALIYUN_OSS_ENDPOINT}
  ALIYUN_OSS_REGION: ${ALIYUN_OSS_REGION}
  ALIYUN_OSS_AUTH_VERSION: ${ALIYUN_OSS_AUTH_VERSION}
  TENCENT_COS_BUCKET_NAME: ${TENCENT_COS_BUCKET_NAME}
  TENCENT_COS_SECRET_KEY: ${TENCENT_COS_SECRET_KEY}
  TENCENT_COS_SECRET_ID: ${TENCENT_COS_SECRET_ID}
  TENCENT_COS_REGION: ${TENCENT_COS_REGION}
  TENCENT_COS_SCHEME: ${TENCENT_COS_SCHEME}
  OCI_ENDPOINT: ${OCI_ENDPOINT}
  OCI_BUCKET_NAME: ${OCI_BUCKET_NAME}
  OCI_ACCESS_KEY: ${OCI_ACCESS_KEY}
  OCI_SECRET_KEY: ${OCI_SECRET_KEY}
  OCI_REGION: ${OCI_REGION}
  VECTOR_STORE: ${VECTOR_STORE}
  WEAVIATE_ENDPOINT: ${WEAVIATE_ENDPOINT}
  WEAVIATE_API_KEY: ${WEAVIATE_API_KEY}
  QDRANT_URL: ${QDRANT_URL}
  QDRANT_API_KEY: ${QDRANT_API_KEY}
  QDRANT_CLIENT_TIMEOUT: ${QDRANT_CLIENT_TIMEOUT}
  QDRANT_GRPC_ENABLED: ${QDRANT_GRPC_ENABLED}
  QDRANT_GRPC_PORT: ${QDRANT_GRPC_PORT}
  MILVUS_HOST: ${MILVUS_HOST}
  MILVUS_PORT: ${MILVUS_PORT}
  MILVUS_USER: ${MILVUS_USER}
  MILVUS_PASSWORD: ${MILVUS_PASSWORD}
  MILVUS_SECURE: ${MILVUS_SECURE}
  MYSCALE_HOST: ${MYSCALE_HOST}
  MYSCALE_PORT: ${MYSCALE_PORT}
  MYSCALE_USER: ${MYSCALE_USER}
  MYSCALE_PASSWORD: ${MYSCALE_PASSWORD}
  MYSCALE_DATABASE: ${MYSCALE_DATABASE}
  MYSCALE_FTS_PARAMS: ${MYSCALE_FTS_PARAMS}
  RELYT_HOST: ${RELYT_HOST}
  RELYT_PORT: ${RELYT_PORT}
  RELYT_USER: ${RELYT_USER}
  RELYT_PASSWORD: ${RELYT_PASSWORD}
  RELYT_DATABASE: ${RELYT_DATABASE}
  PGVECTOR_HOST: ${PGVECTOR_HOST}
  PGVECTOR_PORT: ${PGVECTOR_PORT}
  PGVECTOR_USER: ${PGVECTOR_USER}
  PGVECTOR_PASSWORD: ${PGVECTOR_PASSWORD}
  PGVECTOR_DATABASE: ${PGVECTOR_DATABASE}
  TIDB_VECTOR_HOST: ${TIDB_VECTOR_HOST}
  TIDB_VECTOR_PORT: ${TIDB_VECTOR_PORT}
  TIDB_VECTOR_USER: ${TIDB_VECTOR_USER}
  TIDB_VECTOR_PASSWORD: ${TIDB_VECTOR_PASSWORD}
  TIDB_VECTOR_DATABASE: ${TIDB_VECTOR_DATABASE}
  ORACLE_HOST: ${ORACLE_HOST}
  ORACLE_PORT: ${ORACLE_PORT}
  ORACLE_USER: ${ORACLE_USER}
  ORACLE_PASSWORD: ${ORACLE_PASSWORD}
  ORACLE_DATABASE: ${ORACLE_DATABASE}
  CHROMA_HOST: ${CHROMA_HOST}
  CHROMA_PORT: ${CHROMA_PORT}
  CHROMA_TENANT: ${CHROMA_TENANT}
  CHROMA_DATABASE: ${CHROMA_DATABASE}
  CHROMA_AUTH_PROVIDER: ${CHROMA_AUTH_PROVIDER}
  CHROMA_AUTH_CREDENTIALS: ${CHROMA_AUTH_CREDENTIALS}
  ANALYTICDB_KEY_ID: ${ANALYTICDB_KEY_ID}
  ANALYTICDB_KEY_SECRET: ${ANALYTICDB_KEY_SECRET}
  ANALYTICDB_REGION_ID: ${ANALYTICDB_REGION_ID}
  ANALYTICDB_INSTANCE_ID: ${ANALYTICDB_INSTANCE_ID}
  ANALYTICDB_ACCOUNT: ${ANALYTICDB_ACCOUNT}
  ANALYTICDB_PASSWORD: ${ANALYTICDB_PASSWORD}
  ANALYTICDB_NAMESPACE: ${ANALYTICDB_NAMESPACE}
  ANALYTICDB_NAMESPACE_PASSWORD: ${ANALYTICDB_NAMESPACE_PASSWORD}
  OPENSEARCH_HOST: ${OPENSEARCH_HOST}
  OPENSEARCH_PORT: ${OPENSEARCH_PORT}
  OPENSEARCH_USER: ${OPENSEARCH_USER}
  OPENSEARCH_PASSWORD: ${OPENSEARCH_PASSWORD}
  OPENSEARCH_SECURE: ${OPENSEARCH_SECURE}
  TENCENT_VECTOR_DB_URL: ${TENCENT_VECTOR_DB_URL}
  TENCENT_VECTOR_DB_API_KEY: ${TENCENT_VECTOR_DB_API_KEY}
  TENCENT_VECTOR_DB_TIMEOUT: ${TENCENT_VECTOR_DB_TIMEOUT}
  TENCENT_VECTOR_DB_USERNAME: ${TENCENT_VECTOR_DB_USERNAME}
  TENCENT_VECTOR_DB_DATABASE: ${TENCENT_VECTOR_DB_DATABASE}
  TENCENT_VECTOR_DB_SHARD: ${TENCENT_VECTOR_DB_SHARD}
  TENCENT_VECTOR_DB_REPLICAS: ${TENCENT_VECTOR_DB_REPLICAS}
  UPLOAD_FILE_SIZE_LIMIT: ${UPLOAD_FILE_SIZE_LIMIT}
  UPLOAD_FILE_BATCH_LIMIT: ${UPLOAD_FILE_BATCH_LIMIT}
  ETL_TYPE: ${ETL_TYPE}
  UNSTRUCTURED_API_URL: ${UNSTRUCTURED_API_URL}
  MULTIMODAL_SEND_IMAGE_FORMAT: ${MULTIMODAL_SEND_IMAGE_FORMAT}
  UPLOAD_IMAGE_FILE_SIZE_LIMIT: ${UPLOAD_IMAGE_FILE_SIZE_LIMIT}
  SENTRY_DSN: ${API_SENTRY_DSN}
  SENTRY_TRACES_SAMPLE_RATE: ${API_SENTRY_TRACES_SAMPLE_RATE}
  SENTRY_PROFILES_SAMPLE_RATE: ${API_SENTRY_PROFILES_SAMPLE_RATE}
  NOTION_INTEGRATION_TYPE: ${NOTION_INTEGRATION_TYPE}
  NOTION_CLIENT_SECRET: ${NOTION_CLIENT_SECRET}
  NOTION_CLIENT_ID: ${NOTION_CLIENT_ID}
  NOTION_INTERNAL_SECRET: ${NOTION_INTERNAL_SECRET}
  MAIL_TYPE: ${MAIL_TYPE}
  MAIL_DEFAULT_SEND_FROM: ${MAIL_DEFAULT_SEND_FROM}
  SMTP_SERVER: ${SMTP_SERVER}
  SMTP_PORT: ${SMTP_PORT}
  SMTP_USERNAME: ${SMTP_USERNAME}
  SMTP_PASSWORD: ${SMTP_PASSWORD}
  SMTP_USE_TLS: ${SMTP_USE_TLS}
  SMTP_OPPORTUNISTIC_TLS: ${SMTP_OPPORTUNISTIC_TLS}
  RESEND_API_KEY: ${RESEND_API_KEY}
  RESEND_API_URL: https://api.resend.com
  INDEXING_MAX_SEGMENTATION_TOKENS_LENGTH: ${INDEXING_MAX_SEGMENTATION_TOKENS_LENGTH}
  INVITE_EXPIRY_HOURS: ${INVITE_EXPIRY_HOURS}
  RESET_PASSWORD_TOKEN_EXPIRY_HOURS: ${RESET_PASSWORD_TOKEN_EXPIRY_HOURS}
  CODE_EXECUTION_ENDPOINT: ${CODE_EXECUTION_ENDPOINT}
  CODE_EXECUTION_API_KEY: ${SANDBOX_API_KEY}
  CODE_MAX_NUMBER: ${CODE_MAX_NUMBER}
  CODE_MIN_NUMBER: ${CODE_MIN_NUMBER}
  CODE_MAX_STRING_LENGTH: ${CODE_MAX_STRING_LENGTH}
  TEMPLATE_TRANSFORM_MAX_LENGTH: ${TEMPLATE_TRANSFORM_MAX_LENGTH}
  CODE_MAX_STRING_ARRAY_LENGTH: ${CODE_MAX_STRING_ARRAY_LENGTH}
  CODE_MAX_OBJECT_ARRAY_LENGTH: ${CODE_MAX_OBJECT_ARRAY_LENGTH}
  CODE_MAX_NUMBER_ARRAY_LENGTH: ${CODE_MAX_NUMBER_ARRAY_LENGTH}
  SSRF_PROXY_HTTP_URL: ${SSRF_PROXY_HTTP_URL}
  SSRF_PROXY_HTTPS_URL: ${SSRF_PROXY_HTTPS_URL}

services:
  # API service
  api:
    image: langgenius/dify-api:0.6.15
    restart: always
    environment:
      # Use the shared environment variables.
      <<: *shared-api-worker-env
      # Startup mode, 'api' starts the API server.
      MODE: api
    depends_on:
      - db
      - redis
    volumes:
      # Mount the storage directory to the container, for storing user files.
      - ./volumes/app/storage:/app/api/storage
    networks:
      - ssrf_proxy_network
      - default

  # worker service
  # The Celery worker for processing the queue.
  worker:
    image: langgenius/dify-api:0.6.15
    restart: always
    environment:
      # Use the shared environment variables.
      <<: *shared-api-worker-env
      # Startup mode, 'worker' starts the Celery worker for processing the queue.
      MODE: worker
    depends_on:
      - db
      - redis
    volumes:
      # Mount the storage directory to the container, for storing user files.
      - ./volumes/app/storage:/app/api/storage
    networks:
      - ssrf_proxy_network
      - default

  # Frontend web application.
  web:
    image: langgenius/dify-web:0.6.15
    restart: always
    environment:
      CONSOLE_API_URL: ${CONSOLE_API_URL}
      APP_API_URL: ${APP_API_URL}
      SENTRY_DSN: ${WEB_SENTRY_DSN}

  # The postgres database.
  db:
    image: postgres:15-alpine
    restart: always
    environment:
      PGUSER: ${PGUSER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      PGDATA: ${PGDATA}
    volumes:
      - ./volumes/db/data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready" ]
      interval: 1s
      timeout: 3s
      retries: 30

  # The redis cache.
  redis:
    image: redis:6-alpine
    restart: always
    volumes:
      # Mount the redis data directory to the container.
      - ./volumes/redis/data:/data
    # Set the redis password when startup redis server.
    command: redis-server --requirepass ${REDIS_PASSWORD}
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]

  # The DifySandbox
  sandbox:
    image: langgenius/dify-sandbox:0.2.1
    restart: always
    environment:
      # The DifySandbox configurations
      # Make sure you are changing this key for your deployment with a strong key.
      # You can generate a strong key using `openssl rand -base64 42`.
      API_KEY: ${SANDBOX_API_KEY}
      GIN_MODE: ${SANDBOX_GIN_MODE}
      WORKER_TIMEOUT: ${SANDBOX_WORKER_TIMEOUT}
      ENABLE_NETWORK: ${SANDBOX_ENABLE_NETWORK}
      HTTP_PROXY: ${SANDBOX_HTTP_PROXY}
      HTTPS_PROXY: ${SANDBOX_HTTPS_PROXY}
      SANDBOX_PORT: ${SANDBOX_PORT}
    volumes:
      - ./volumes/sandbox/dependencies:/dependencies
    networks:
      - ssrf_proxy_network

  # ssrf_proxy server
  ssrf_proxy:
    image: ubuntu/squid:latest
    restart: always
    volumes:
      - ./ssrf_proxy/squid.conf.template:/etc/squid/squid.conf.template
      - ./ssrf_proxy/docker-entrypoint.sh:/docker-entrypoint-mount.sh
    entrypoint: [ "sh", "-c", "cp /docker-entrypoint-mount.sh /docker-entrypoint.sh && sed -i 's/\r$$//' /docker-entrypoint.sh && chmod +x /docker-entrypoint.sh && /docker-entrypoint.sh" ]
    environment:
      HTTP_PORT: ${SSRF_HTTP_PORT}
      COREDUMP_DIR: ${SSRF_COREDUMP_DIR}
      REVERSE_PROXY_PORT: ${SSRF_REVERSE_PROXY_PORT}
      SANDBOX_HOST: ${SSRF_SANDBOX_HOST}
      SANDBOX_PORT: ${SANDBOX_PORT}
    networks:
      - ssrf_proxy_network
      - default

  # Certbot service
  certbot:
    image: certbot/certbot
    profiles:
      - certbot
    volumes:
      - ./volumes/certbot/conf:/etc/letsencrypt
      - ./volumes/certbot/www:/var/www/html
      - ./volumes/certbot/logs:/var/log/letsencrypt
      - ./volumes/certbot/conf/live:/etc/letsencrypt/live
      - ./certbot/update-cert.template.txt:/update-cert.template.txt
      - ./certbot/docker-entrypoint.sh:/docker-entrypoint.sh
    environment:
      - CERTBOT_EMAIL=${CERTBOT_EMAIL}
      - CERTBOT_DOMAIN=${CERTBOT_DOMAIN}
      - CERTBOT_OPTIONS=${CERTBOT_OPTIONS}
    entrypoint: [ "/docker-entrypoint.sh" ]
    command: ["tail", "-f", "/dev/null"]

  # The nginx reverse proxy.
  nginx:
    image: nginx:latest
    restart: always
    volumes:
      - ./nginx/nginx.conf.template:/etc/nginx/nginx.conf.template
      - ./nginx/proxy.conf.template:/etc/nginx/proxy.conf.template
      - ./nginx/https.conf.template:/etc/nginx/https.conf.template
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./nginx/docker-entrypoint.sh:/docker-entrypoint-mount.sh
      - ./nginx/ssl:/etc/ssl
      - ./volumes/certbot/conf/live:/etc/letsencrypt/live
      - ./volumes/certbot/conf:/etc/letsencrypt
      - ./volumes/certbot/www:/var/www/html
    entrypoint: [ "sh", "-c", "cp /docker-entrypoint-mount.sh /docker-entrypoint.sh && sed -i 's/\r$$//' /docker-entrypoint.sh && chmod +x /docker-entrypoint.sh && /docker-entrypoint.sh" ]
    environment:
      NGINX_SERVER_NAME: ${NGINX_SERVER_NAME}
      NGINX_HTTPS_ENABLED: ${NGINX_HTTPS_ENABLED}
      NGINX_SSL_PORT: ${NGINX_SSL_PORT}
      NGINX_PORT: ${NGINX_PORT}
      NGINX_SSL_CERT_FILENAME: ${NGINX_SSL_CERT_FILENAME}
      NGINX_SSL_CERT_KEY_FILENAME: ${NGINX_SSL_CERT_KEY_FILENAME}
      NGINX_SSL_PROTOCOLS: ${NGINX_SSL_PROTOCOLS}
      NGINX_WORKER_PROCESSES: ${NGINX_WORKER_PROCESSES}
      NGINX_CLIENT_MAX_BODY_SIZE: ${NGINX_CLIENT_MAX_BODY_SIZE}
      NGINX_KEEPALIVE_TIMEOUT: ${NGINX_KEEPALIVE_TIMEOUT}
      NGINX_PROXY_READ_TIMEOUT: ${NGINX_PROXY_READ_TIMEOUT}
      NGINX_PROXY_SEND_TIMEOUT: ${NGINX_PROXY_SEND_TIMEOUT}
      NGINX_ENABLE_CERTBOT_CHALLENGE: ${NGINX_ENABLE_CERTBOT_CHALLENGE}
      CERTBOT_DOMAIN: ${CERTBOT_DOMAIN}
    depends_on:
      - api
      - web
    ports:
      - "${EXPOSE_NGINX_PORT}:${NGINX_PORT}"
      - "${EXPOSE_NGINX_SSL_PORT}:${NGINX_SSL_PORT}"

  # The Weaviate vector store.
  weaviate:
    image: semitechnologies/weaviate:1.19.0
    profiles:
      - ''
      - weaviate
    restart: always
    volumes:
      - ./volumes/weaviate:/var/lib/weaviate
    environment:
      PERSISTENCE_DATA_PATH: ${WEAVIATE_PERSISTENCE_DATA_PATH}
      QUERY_DEFAULTS_LIMIT: ${WEAVIATE_QUERY_DEFAULTS_LIMIT}
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: ${WEAVIATE_AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED}
      DEFAULT_VECTORIZER_MODULE: ${WEAVIATE_DEFAULT_VECTORIZER_MODULE}
      CLUSTER_HOSTNAME: ${WEAVIATE_CLUSTER_HOSTNAME}
      AUTHENTICATION_APIKEY_ENABLED: ${WEAVIATE_AUTHENTICATION_APIKEY_ENABLED}
      AUTHENTICATION_APIKEY_ALLOWED_KEYS: ${WEAVIATE_AUTHENTICATION_APIKEY_ALLOWED_KEYS}
      AUTHENTICATION_APIKEY_USERS: ${WEAVIATE_AUTHENTICATION_APIKEY_USERS}
      AUTHORIZATION_ADMINLIST_ENABLED: ${WEAVIATE_AUTHORIZATION_ADMINLIST_ENABLED}
      AUTHORIZATION_ADMINLIST_USERS: ${WEAVIATE_AUTHORIZATION_ADMINLIST_USERS}

  # Qdrant vector store.
  qdrant:
    image: langgenius/qdrant:v1.7.3
    profiles:
      - qdrant
    restart: always
    volumes:
      - ./volumes/qdrant:/qdrant/storage
    environment:
      QDRANT_API_KEY: ${QDRANT_API_KEY}

  # The pgvector vector database.
  pgvector:
    image: pgvector/pgvector:pg16
    profiles:
      - pgvector
    restart: always
    environment:
      PGUSER: ${PGVECTOR_PGUSER}
      POSTGRES_PASSWORD: ${PGVECTOR_POSTGRES_PASSWORD}
      POSTGRES_DB: ${PGVECTOR_POSTGRES_DB}
      PGDATA: ${PGVECTOR_PGDATA}
    volumes:
      - ./volumes/pgvector/data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready" ]
      interval: 1s
      timeout: 3s
      retries: 30

  # pgvecto-rs vector store
  pgvecto-rs:
    image: tensorchord/pgvecto-rs:pg16-v0.2.0
    profiles:
      - pgvecto-rs
    restart: always
    environment:
      PGUSER: ${PGVECTOR_PGUSER}
      POSTGRES_PASSWORD: ${PGVECTOR_POSTGRES_PASSWORD}
      POSTGRES_DB: ${PGVECTOR_POSTGRES_DB}
      PGDATA: ${PGVECTOR_PGDATA}
    volumes:
      - ./volumes/pgvecto_rs/data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready" ]
      interval: 1s
      timeout: 3s
      retries: 30

  # Chroma vector database
  chroma:
    image: ghcr.io/chroma-core/chroma:0.5.1
    profiles:
      - chroma
    restart: always
    volumes:
      - ./volumes/chroma:/chroma/chroma
    environment:
      CHROMA_SERVER_AUTHN_CREDENTIALS: ${CHROMA_SERVER_AUTHN_CREDENTIALS}
      CHROMA_SERVER_AUTHN_PROVIDER: ${CHROMA_SERVER_AUTHN_PROVIDER}
      IS_PERSISTENT: ${CHROMA_IS_PERSISTENT}

  # Oracle vector database
  oracle:
    image: container-registry.oracle.com/database/free:latest
    profiles:
      - oracle
    restart: always
    volumes:
      - type: volume
        source: oradata
        target: /opt/oracle/oradata
      - ./startupscripts:/opt/oracle/scripts/startup
    environment:
      - ORACLE_PWD=${ORACLE_PWD}
      - ORACLE_CHARACTERSET=${ORACLE_CHARACTERSET}

  # Milvus vector database services
  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.5
    profiles:
      - milvus
    environment:
      - ETCD_AUTO_COMPACTION_MODE=${ETCD_AUTO_COMPACTION_MODE}
      - ETCD_AUTO_COMPACTION_RETENTION=${ETCD_AUTO_COMPACTION_RETENTION}
      - ETCD_QUOTA_BACKEND_BYTES=${ETCD_QUOTA_BACKEND_BYTES}
      - ETCD_SNAPSHOT_COUNT=${ETCD_SNAPSHOT_COUNT}
    volumes:
      - ./volumes/milvus/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: [ "CMD", "etcdctl", "endpoint", "health" ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - milvus

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    profiles:
      - milvus
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    volumes:
      - ./volumes/milvus/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - milvus

  milvus-standalone:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.3.1
    profiles:
      - milvus
    command: [ "milvus", "run", "standalone" ]
    environment:
      ETCD_ENDPOINTS: ${ETCD_ENDPOINTS}
      MINIO_ADDRESS: ${MINIO_ADDRESS}
      common.security.authorizationEnabled: ${MILVUS_AUTHORIZATION_ENABLED}
    volumes:
      - ./volumes/milvus/milvus:/var/lib/milvus
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9091/healthz" ]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    depends_on:
      - "etcd"
      - "minio"
    networks:
      - milvus

  # Opensearch vector database
  opensearch:
    container_name: opensearch
    image: opensearchproject/opensearch:latest
    profiles:
      - opensearch
    environment:
      - discovery.type=${OPENSEARCH_DISCOVERY_TYPE}
      - bootstrap.memory_lock=${OPENSEARCH_BOOTSTRAP_MEMORY_LOCK}
      - OPENSEARCH_JAVA_OPTS=-Xms${OPENSEARCH_JAVA_OPTS_MIN} -Xmx${OPENSEARCH_JAVA_OPTS_MAX}
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_INITIAL_ADMIN_PASSWORD}
    ulimits:
      memlock:
        soft: ${OPENSEARCH_MEMLOCK_SOFT}
        hard: ${OPENSEARCH_MEMLOCK_HARD}
      nofile:
        soft: ${OPENSEARCH_NOFILE_SOFT}
        hard: ${OPENSEARCH_NOFILE_HARD}
    volumes:
      - ./volumes/opensearch/data:/usr/share/opensearch/data
    networks:
      - opensearch-net

  opensearch-dashboards:
    container_name: opensearch-dashboards
    image: opensearchproject/opensearch-dashboards:latest
    profiles:
      - opensearch
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch:9200"]'
    volumes:
      - ./volumes/opensearch/opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml
    networks:
      - opensearch-net
    depends_on:
      - opensearch

  # MyScale vector database
  myscale:
    container_name: myscale
    image: myscale/myscaledb:1.6
    profiles:
      - myscale
    restart: always
    tty: true
    volumes:
      - ./volumes/myscale/data:/var/lib/clickhouse
      - ./volumes/myscale/log:/var/log/clickhouse-server
      - ./volumes/myscale/config/users.d/custom_users_config.xml:/etc/clickhouse-server/users.d/custom_users_config.xml
    ports:
      - "${MYSCALE_PORT}:${MYSCALE_PORT}"

  # unstructured .
  # (if used, you need to set ETL_TYPE to Unstructured in the api & worker service.)
  unstructured:
    image: downloads.unstructured.io/unstructured-io/unstructured-api:latest
    profiles:
      - unstructured
    restart: always
    volumes:
      - ./volumes/unstructured:/app/data

networks:
  # create a network between sandbox, api and ssrf_proxy, and can not access outside.
  ssrf_proxy_network:
    driver: bridge
    internal: true
  milvus:
    driver: bridge
  opensearch-net:
    driver: bridge
    internal: true

volumes:
  oradata:
